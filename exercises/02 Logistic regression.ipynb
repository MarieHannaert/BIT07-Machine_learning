{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "The goal of logistic regression is to classify data using a trained model. Logistic regression is a simple algorithm that does not require a lot of computational power (in contrast to techniques like support vector machines) whilest often perform as well (or even better) than more complex models. Besides just assigning a class, it also calculates the probabilities allowing us to see how confident the model is in a decission.\n",
    "\n",
    "The goals of this exercise are\n",
    "* Correctly training and tuning a logistic regression classifier\n",
    "* Performing classification via logistic regression\n",
    "* Perform feature engineering\n",
    "* Interpretting the different performance metrics like accuracy, recall, precision, f1-score, ROC\n",
    "* Know when you're dealing with under- and overfitting and the ability to make adaptions regarding this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt                        # To create plots\n",
    "import numpy as np                                     # To perform calculations quickly\n",
    "import pandas as pd                                    # To load in and manipulate data\n",
    "from sklearn.linear_model import LogisticRegression    # Linear model\n",
    "from sklearn.model_selection import train_test_split   # Split up the data in a train and test set\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,recall_score,precision_score,f1_score \n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breastcancer dataset\n",
    "\n",
    "https://www.kaggle.com/pranaykankariya/breastcancer-dataset\n",
    "\n",
    "Target column: diagnosis (Malign or Benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Take a look at the first rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a countplot to determine if the classes are balanced\n",
    "# Use sns.countplot(data=df_cancer,x='column_of_interest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove uninformative columns (look at the summary and first rows to determine which columns you should remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot of the different features\n",
    "# This can be done using the pandas hist function.\n",
    "# => my_df.hist()\n",
    "# To increase the size of the figure, you can pass a figsize tupple. (20,15) works good. You can also hide the grid\n",
    "# with the option \"grid=False\"\n",
    "# => my_df.hist(figsize=(20,15),grid=False)\n",
    "\n",
    "#To hide the plot information you can assign it to a variable, or add a \";\" at the end (other options exist)\n",
    "# => plot = my_df.hist(figsize=(20,15),grid=False)         or       my_df.hist(figsize=(20,15),grid=False);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and targets (or X, y depending on your preference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into a training and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data, use a scaler. You can base your choice on the histogrammes above, or by just trying them out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a logistic regression model, fit the data. Start with a C-value of 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "In the next section we will evaluate our model using different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have over- or underfitting of your model by comparing the score of the training and test set\n",
    "\n",
    "\n",
    "# Predict values for the test set\n",
    "\n",
    "# Look at the confusion matrix, what do the different values mean in this case?\n",
    "# Hint: if you don't know the syntax/meaning for a specific funtion, you can always look this up\n",
    "# in jupyter notebook by executing \"?function_name\"\n",
    "\n",
    "\n",
    "# Show the accuracy, recall, precision and f1-score for the test set\n",
    "# Note, sometimes you need to supply a positive label (if not working with 0 and 1)\n",
    "# supply this with \"pos_label='label'\", in this case, the malign samples are the positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a roc curve, also show the AUROC\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some different C-values for the model. E.g. 0.0001 and 1000\n",
    "\n",
    "What do you see in the metrics? what does this mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "The attributes are:\n",
    "1. Alcohol\n",
    "2. Malic acid\n",
    "3. Ash\n",
    "4. Alcalinity of ash\n",
    "5. Magnesium\n",
    "6. Total phenols\n",
    "7. Flavanoids\n",
    "8. Nonflavanoid phenols\n",
    "9. Proanthocyanins\n",
    "10. Color intensity\n",
    "11. Hue\n",
    "12. OD280/OD315 of diluted wines\n",
    "13. Proline\n",
    "\n",
    "The feature of interest is \"cultivar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "# Take a look at the first rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a summary of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and targets (X and y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test set, keep about 20% of the data to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale/normalize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "\n",
    "# Check if you have over- or underfitting of your model by comparing the score of the training and test set\n",
    "\n",
    "\n",
    "# Predict values for the test set\n",
    "\n",
    "\n",
    "# Look at the confusion matrix, what do the different values mean in this case\n",
    "\n",
    "# Show the accuracy, recall, precision and f1-score for the test set\n",
    "# Note, since we have multiple classes, we have to provide an average parameter to recall, precision and f1-score\n",
    "# Using average=None will result in the scores for all classes. To know which classes correspond to which values,\n",
    "# you can take a look at model.classes_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
